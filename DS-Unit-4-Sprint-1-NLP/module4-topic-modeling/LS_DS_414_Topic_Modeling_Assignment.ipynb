{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Topic Modeling\n",
    "## *Data Science Unit 4 Sprint 1 Assignment 4*\n",
    "\n",
    "Analyze a corpus of Amazon reviews from Unit 4 Sprint 1 Module 1's lecture using topic modeling: \n",
    "\n",
    "- Fit a Gensim LDA topic model on Amazon Reviews\n",
    "- Select appropriate number of topics\n",
    "- Create some dope visualization of the topics\n",
    "- Write a few bullets on your findings in markdown at the end\n",
    "- **Note**: You don't *have* to use generators for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Gensim LDA topic model on Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "from tswift import Artist, Song\n",
    "\n",
    "# NLP\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# Data Structures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standard Library\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text data\n",
    "# Make a doc_stream() generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def doc_stream(path):\n",
    "#     for f in os.listdir(path):\n",
    "#         with open(os.path.join(path,f)) as t:\n",
    "#             text = t.read().strip('\\n')\n",
    "#             # tokens = tokenize(str(text))\n",
    "#             yield tokenize(str(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "* Incorporate Named Entity Recognition in your analysis\n",
    "* Incorporate some custom pre-processing from our previous lessons (like spacy lemmatization)\n",
    "* Analyze a dataset of interest to you with topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python 3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
